{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:-\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to manipulate the data\n",
    "import matplotlib.pyplot as plt # to visualise the result of our model\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input #to create the Embedding matrix\n",
    "from tensorflow.keras.layers import LSTM # predicting using the Long Short Term Memory model\n",
    "from tensorflow.keras.layers import Dropout #for the regularization(handling the overfitting)\n",
    "from tensorflow.keras.layers import Dense #to create the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 100000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'deu-eng/deu.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing the data\n",
    "input_texts = [] # creating empty list to add the input sentences\n",
    "target_texts = [] # creating empty list to add the target sentences\n",
    "input_characters = set() # creating empty set to add the input language charecters(since we need only the unique charecter so we use set)\n",
    "target_characters = set() # creating empty set to add the target language charecters(since we need only the unique charecter so we use set)\n",
    "with open(data_path, 'r' , encoding = 'utf-8') as f: # reading the file from the directory\n",
    "    lines = f.read().split('\\n') # reading each line of the code\n",
    "# we need to separate the english and german so we use for loop\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]: # we are going to take only 100000 sentences not more than that\n",
    "    input_text, target_text, _ = line.split(\"\\t\") #since the english statement and german is separated by tab and remaining sentence also by tab so we are spliting with tab\n",
    "    # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text) # appending the input text to the to the list\n",
    "    target_texts.append(target_text) # appending the target text to the list\n",
    "    for char in input_text:\n",
    "        if char not in input_characters: # taking only the unique charecter of the input language(english)\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters: # taking only the unique charecter of the target language(german)\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the set into list and sorting the charecters\n",
    "input_characters = sorted(list(input_characters)) \n",
    "target_characters = sorted(list(target_characters))\n",
    "# calculating the length of total charecter in the input and target language\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "#calculating the length of the longest sentence in both input and the output language\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique input tokens: 80\n",
      "Number of unique output tokens: 104\n",
      "Max sequence length for inputs: 28\n",
      "Max sequence length for outputs: 122\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(input_texts)) # total sample of text that we are taking under consideration\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens) # length of charecters in input language\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens) # length of charecters in target language\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length) # words in the longest sentence in input language\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length) # words in the longest sentence in target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the charecter like assigning 0 to first charecter and 1 to second and so on\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)]) \n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " \"'\": 5,\n",
       " '+': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '/': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " '?': 22,\n",
       " 'A': 23,\n",
       " 'B': 24,\n",
       " 'C': 25,\n",
       " 'D': 26,\n",
       " 'E': 27,\n",
       " 'F': 28,\n",
       " 'G': 29,\n",
       " 'H': 30,\n",
       " 'I': 31,\n",
       " 'J': 32,\n",
       " 'K': 33,\n",
       " 'L': 34,\n",
       " 'M': 35,\n",
       " 'N': 36,\n",
       " 'O': 37,\n",
       " 'P': 38,\n",
       " 'Q': 39,\n",
       " 'R': 40,\n",
       " 'S': 41,\n",
       " 'T': 42,\n",
       " 'U': 43,\n",
       " 'V': 44,\n",
       " 'W': 45,\n",
       " 'Y': 46,\n",
       " 'Z': 47,\n",
       " 'a': 48,\n",
       " 'b': 49,\n",
       " 'c': 50,\n",
       " 'd': 51,\n",
       " 'e': 52,\n",
       " 'f': 53,\n",
       " 'g': 54,\n",
       " 'h': 55,\n",
       " 'i': 56,\n",
       " 'j': 57,\n",
       " 'k': 58,\n",
       " 'l': 59,\n",
       " 'm': 60,\n",
       " 'n': 61,\n",
       " 'o': 62,\n",
       " 'p': 63,\n",
       " 'q': 64,\n",
       " 'r': 65,\n",
       " 's': 66,\n",
       " 't': 67,\n",
       " 'u': 68,\n",
       " 'v': 69,\n",
       " 'w': 70,\n",
       " 'x': 71,\n",
       " 'y': 72,\n",
       " 'z': 73,\n",
       " '\\xa0': 74,\n",
       " 'é': 75,\n",
       " 'ï': 76,\n",
       " 'ñ': 77,\n",
       " '’': 78,\n",
       " '€': 79}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the required input and the output to the encoder and decoder. Initiating all the arrays with zero\n",
    "#len(input_texts) = total no sentences in the input language\n",
    "#max_encoder_seq_length = longest sentence in the input language\n",
    "#num_encoder_tokens = number of charecter in the input language\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot representation\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): # 'i' will give the count and input_text = input_texts[i] and target_text = target_texts[i](extracting the sentence one by one)\n",
    "    for t, char in enumerate(input_text): # 't' is the count and char will get sentence in the input_text[0](extracting the charecter one by one)\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0 # assigning 1 to the charecter\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0 # after each word where ever there is space(' ') then also assigning 1\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Decoder_target_data will result first from the context vector provided by the encoder and the same output will go to the input of the next decoder so we are adding the offset as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an input sequence and processing it.\n",
    "encoder_inputs = tensorflow.keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = tensorflow.keras.layers.LSTM(latent_dim, return_state=True) # since we don't need the output  so return_state = True\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs) # adding all the outcomes of encoder in the new variable\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = tensorflow.keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = tensorflow.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True) #defining the LSTM for the decoder\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) # keeping the output of the decoder\n",
    "decoder_dense = tensorflow.keras.layers.Dense(num_decoder_tokens, activation=\"softmax\") #dense layer to get the combined output sentence\n",
    "decoder_outputs = decoder_dense(decoder_outputs) # passing all the out of the decoder to get the complete sentence\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = tensorflow.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "80000/80000 [==============================] - 40s 505us/sample - loss: 0.4454 - accuracy: 0.8772 - val_loss: 0.4375 - val_accuracy: 0.8708\n",
      "Epoch 2/100\n",
      "80000/80000 [==============================] - 29s 364us/sample - loss: 0.2866 - accuracy: 0.9151 - val_loss: 0.3552 - val_accuracy: 0.8941\n",
      "Epoch 3/100\n",
      "80000/80000 [==============================] - 29s 358us/sample - loss: 0.2441 - accuracy: 0.9273 - val_loss: 0.3185 - val_accuracy: 0.9046\n",
      "Epoch 4/100\n",
      "80000/80000 [==============================] - 30s 374us/sample - loss: 0.2203 - accuracy: 0.9344 - val_loss: 0.2991 - val_accuracy: 0.9107\n",
      "Epoch 5/100\n",
      "80000/80000 [==============================] - 30s 379us/sample - loss: 0.2051 - accuracy: 0.9390 - val_loss: 0.2862 - val_accuracy: 0.9146\n",
      "Epoch 6/100\n",
      "80000/80000 [==============================] - 31s 381us/sample - loss: 0.1945 - accuracy: 0.9421 - val_loss: 0.2750 - val_accuracy: 0.9178\n",
      "Epoch 7/100\n",
      "80000/80000 [==============================] - 31s 384us/sample - loss: 0.1857 - accuracy: 0.9448 - val_loss: 0.2675 - val_accuracy: 0.9204\n",
      "Epoch 8/100\n",
      "80000/80000 [==============================] - 30s 378us/sample - loss: 0.1782 - accuracy: 0.9469 - val_loss: 0.2619 - val_accuracy: 0.9224\n",
      "Epoch 9/100\n",
      "80000/80000 [==============================] - 30s 375us/sample - loss: 0.1718 - accuracy: 0.9488 - val_loss: 0.2573 - val_accuracy: 0.9236\n",
      "Epoch 10/100\n",
      "80000/80000 [==============================] - 30s 378us/sample - loss: 0.1663 - accuracy: 0.9505 - val_loss: 0.2541 - val_accuracy: 0.9250\n",
      "Epoch 11/100\n",
      "80000/80000 [==============================] - 30s 378us/sample - loss: 0.1614 - accuracy: 0.9519 - val_loss: 0.2523 - val_accuracy: 0.9257\n",
      "Epoch 12/100\n",
      "80000/80000 [==============================] - 30s 377us/sample - loss: 0.1570 - accuracy: 0.9533 - val_loss: 0.2485 - val_accuracy: 0.9269\n",
      "Epoch 13/100\n",
      "80000/80000 [==============================] - 30s 380us/sample - loss: 0.1531 - accuracy: 0.9544 - val_loss: 0.2470 - val_accuracy: 0.9276\n",
      "Epoch 14/100\n",
      "80000/80000 [==============================] - 30s 381us/sample - loss: 0.1497 - accuracy: 0.9554 - val_loss: 0.2461 - val_accuracy: 0.9282\n",
      "Epoch 15/100\n",
      "80000/80000 [==============================] - 31s 381us/sample - loss: 0.1465 - accuracy: 0.9564 - val_loss: 0.2446 - val_accuracy: 0.9288\n",
      "Epoch 16/100\n",
      "80000/80000 [==============================] - 30s 379us/sample - loss: 0.1436 - accuracy: 0.9572 - val_loss: 0.2446 - val_accuracy: 0.9289\n",
      "Epoch 17/100\n",
      "80000/80000 [==============================] - 31s 385us/sample - loss: 0.1409 - accuracy: 0.9580 - val_loss: 0.2433 - val_accuracy: 0.9297\n",
      "Epoch 18/100\n",
      "80000/80000 [==============================] - 30s 378us/sample - loss: 0.1386 - accuracy: 0.9587 - val_loss: 0.2449 - val_accuracy: 0.9294\n",
      "Epoch 19/100\n",
      "80000/80000 [==============================] - 31s 382us/sample - loss: 0.1362 - accuracy: 0.9594 - val_loss: 0.2435 - val_accuracy: 0.9301\n",
      "Epoch 20/100\n",
      "80000/80000 [==============================] - 32s 401us/sample - loss: 0.1341 - accuracy: 0.9600 - val_loss: 0.2438 - val_accuracy: 0.9302\n",
      "Epoch 21/100\n",
      "80000/80000 [==============================] - 32s 396us/sample - loss: 0.1322 - accuracy: 0.9605 - val_loss: 0.2435 - val_accuracy: 0.9304\n",
      "Epoch 22/100\n",
      "80000/80000 [==============================] - 31s 386us/sample - loss: 0.1303 - accuracy: 0.9611 - val_loss: 0.2447 - val_accuracy: 0.9303\n",
      "Epoch 23/100\n",
      "80000/80000 [==============================] - 31s 382us/sample - loss: 0.1285 - accuracy: 0.9617 - val_loss: 0.2451 - val_accuracy: 0.9306\n",
      "Epoch 24/100\n",
      "80000/80000 [==============================] - 31s 389us/sample - loss: 0.1268 - accuracy: 0.9621 - val_loss: 0.2445 - val_accuracy: 0.9307\n",
      "Epoch 25/100\n",
      "80000/80000 [==============================] - 31s 387us/sample - loss: 0.1252 - accuracy: 0.9626 - val_loss: 0.2468 - val_accuracy: 0.9305\n",
      "Epoch 26/100\n",
      "80000/80000 [==============================] - 30s 381us/sample - loss: 0.1237 - accuracy: 0.9630 - val_loss: 0.2468 - val_accuracy: 0.9305\n",
      "Epoch 27/100\n",
      "80000/80000 [==============================] - 31s 382us/sample - loss: 0.1222 - accuracy: 0.9635 - val_loss: 0.2483 - val_accuracy: 0.9306\n",
      "Epoch 28/100\n",
      "80000/80000 [==============================] - 31s 383us/sample - loss: 0.1209 - accuracy: 0.9639 - val_loss: 0.2498 - val_accuracy: 0.9304\n",
      "Epoch 29/100\n",
      "80000/80000 [==============================] - 31s 383us/sample - loss: 0.1196 - accuracy: 0.9642 - val_loss: 0.2511 - val_accuracy: 0.9303\n",
      "Epoch 30/100\n",
      "80000/80000 [==============================] - 30s 381us/sample - loss: 0.1183 - accuracy: 0.9646 - val_loss: 0.2511 - val_accuracy: 0.9305\n",
      "Epoch 31/100\n",
      "80000/80000 [==============================] - 31s 388us/sample - loss: 0.1171 - accuracy: 0.9649 - val_loss: 0.2510 - val_accuracy: 0.9308\n",
      "Epoch 32/100\n",
      "80000/80000 [==============================] - 30s 379us/sample - loss: 0.1160 - accuracy: 0.9653 - val_loss: 0.2530 - val_accuracy: 0.9304\n",
      "Epoch 33/100\n",
      "80000/80000 [==============================] - 30s 379us/sample - loss: 0.1151 - accuracy: 0.9655 - val_loss: 0.2541 - val_accuracy: 0.9305\n",
      "Epoch 34/100\n",
      "80000/80000 [==============================] - 30s 381us/sample - loss: 0.1139 - accuracy: 0.9658 - val_loss: 0.2549 - val_accuracy: 0.9303\n",
      "Epoch 35/100\n",
      "80000/80000 [==============================] - 31s 388us/sample - loss: 0.1128 - accuracy: 0.9662 - val_loss: 0.2564 - val_accuracy: 0.9301\n",
      "Epoch 36/100\n",
      "80000/80000 [==============================] - 31s 382us/sample - loss: 0.1118 - accuracy: 0.9664 - val_loss: 0.2579 - val_accuracy: 0.9301y: 0.96\n",
      "Epoch 37/100\n",
      "80000/80000 [==============================] - 30s 380us/sample - loss: 0.1108 - accuracy: 0.9667 - val_loss: 0.2589 - val_accuracy: 0.9300\n",
      "Epoch 38/100\n",
      "80000/80000 [==============================] - 32s 397us/sample - loss: 0.1099 - accuracy: 0.9670 - val_loss: 0.2602 - val_accuracy: 0.9296\n",
      "Epoch 39/100\n",
      "80000/80000 [==============================] - 32s 397us/sample - loss: 0.1090 - accuracy: 0.9672 - val_loss: 0.2612 - val_accuracy: 0.9297\n",
      "Epoch 40/100\n",
      "80000/80000 [==============================] - 32s 401us/sample - loss: 0.1082 - accuracy: 0.9674 - val_loss: 0.2624 - val_accuracy: 0.9295\n",
      "Epoch 41/100\n",
      "80000/80000 [==============================] - 33s 412us/sample - loss: 0.1074 - accuracy: 0.9677 - val_loss: 0.2636 - val_accuracy: 0.9297\n",
      "Epoch 42/100\n",
      "80000/80000 [==============================] - 32s 399us/sample - loss: 0.1066 - accuracy: 0.9679 - val_loss: 0.2658 - val_accuracy: 0.9294\n",
      "Epoch 43/100\n",
      "80000/80000 [==============================] - 32s 402us/sample - loss: 0.1058 - accuracy: 0.9681 - val_loss: 0.2661 - val_accuracy: 0.9292\n",
      "Epoch 44/100\n",
      "80000/80000 [==============================] - 32s 397us/sample - loss: 0.1051 - accuracy: 0.9683 - val_loss: 0.2671 - val_accuracy: 0.9292\n",
      "Epoch 45/100\n",
      "80000/80000 [==============================] - 32s 400us/sample - loss: 0.1044 - accuracy: 0.9685 - val_loss: 0.2686 - val_accuracy: 0.9291\n",
      "Epoch 46/100\n",
      "80000/80000 [==============================] - 32s 405us/sample - loss: 0.1036 - accuracy: 0.9687 - val_loss: 0.2708 - val_accuracy: 0.9287\n",
      "Epoch 47/100\n",
      "80000/80000 [==============================] - 32s 398us/sample - loss: 0.1030 - accuracy: 0.9689 - val_loss: 0.2700 - val_accuracy: 0.9290\n",
      "Epoch 48/100\n",
      "80000/80000 [==============================] - 32s 398us/sample - loss: 0.1023 - accuracy: 0.9691 - val_loss: 0.2728 - val_accuracy: 0.9287\n",
      "Epoch 49/100\n",
      "80000/80000 [==============================] - 32s 401us/sample - loss: 0.1017 - accuracy: 0.9692 - val_loss: 0.2739 - val_accuracy: 0.9285\n",
      "Epoch 50/100\n",
      "80000/80000 [==============================] - 32s 401us/sample - loss: 0.1010 - accuracy: 0.9694 - val_loss: 0.2747 - val_accuracy: 0.9285\n",
      "Epoch 51/100\n",
      "80000/80000 [==============================] - 32s 397us/sample - loss: 0.1005 - accuracy: 0.9695 - val_loss: 0.2754 - val_accuracy: 0.9285\n",
      "Epoch 52/100\n",
      "80000/80000 [==============================] - 32s 401us/sample - loss: 0.1000 - accuracy: 0.9697 - val_loss: 0.2771 - val_accuracy: 0.9283\n",
      "Epoch 53/100\n",
      "80000/80000 [==============================] - 33s 408us/sample - loss: 0.0994 - accuracy: 0.9698 - val_loss: 0.2771 - val_accuracy: 0.9283\n",
      "Epoch 54/100\n",
      "80000/80000 [==============================] - 33s 415us/sample - loss: 0.0989 - accuracy: 0.9700 - val_loss: 0.2788 - val_accuracy: 0.9280\n",
      "Epoch 55/100\n",
      "80000/80000 [==============================] - 33s 407us/sample - loss: 0.0984 - accuracy: 0.9702 - val_loss: 0.2786 - val_accuracy: 0.9284\n",
      "Epoch 56/100\n",
      "80000/80000 [==============================] - 33s 410us/sample - loss: 0.0978 - accuracy: 0.9703 - val_loss: 0.2806 - val_accuracy: 0.9282\n",
      "Epoch 57/100\n",
      "80000/80000 [==============================] - 33s 407us/sample - loss: 0.0973 - accuracy: 0.9705 - val_loss: 0.2805 - val_accuracy: 0.9280\n",
      "Epoch 58/100\n",
      "80000/80000 [==============================] - 33s 418us/sample - loss: 0.0969 - accuracy: 0.9706 - val_loss: 0.2835 - val_accuracy: 0.9279\n",
      "Epoch 59/100\n",
      "80000/80000 [==============================] - 32s 403us/sample - loss: 0.0964 - accuracy: 0.9707 - val_loss: 0.2848 - val_accuracy: 0.9276\n",
      "Epoch 60/100\n",
      "80000/80000 [==============================] - 32s 402us/sample - loss: 0.0960 - accuracy: 0.9708 - val_loss: 0.2846 - val_accuracy: 0.9278\n",
      "Epoch 61/100\n",
      "80000/80000 [==============================] - 33s 407us/sample - loss: 0.0954 - accuracy: 0.9710 - val_loss: 0.2864 - val_accuracy: 0.9275\n",
      "Epoch 62/100\n",
      "80000/80000 [==============================] - 32s 405us/sample - loss: 0.0951 - accuracy: 0.9711 - val_loss: 0.2865 - val_accuracy: 0.9276\n",
      "Epoch 63/100\n",
      "80000/80000 [==============================] - 33s 407us/sample - loss: 0.0946 - accuracy: 0.9712 - val_loss: 0.2883 - val_accuracy: 0.9276\n",
      "Epoch 64/100\n",
      "80000/80000 [==============================] - 33s 409us/sample - loss: 0.0942 - accuracy: 0.9713 - val_loss: 0.2887 - val_accuracy: 0.9274\n",
      "Epoch 65/100\n",
      "80000/80000 [==============================] - 32s 402us/sample - loss: 0.0938 - accuracy: 0.9714 - val_loss: 0.2892 - val_accuracy: 0.9275\n",
      "Epoch 66/100\n",
      "80000/80000 [==============================] - 33s 414us/sample - loss: 0.0936 - accuracy: 0.9715 - val_loss: 0.2916 - val_accuracy: 0.9272\n",
      "Epoch 67/100\n",
      "80000/80000 [==============================] - 32s 402us/sample - loss: 0.0945 - accuracy: 0.9712 - val_loss: 0.2920 - val_accuracy: 0.9270\n",
      "Epoch 68/100\n",
      "80000/80000 [==============================] - 33s 406us/sample - loss: 0.0927 - accuracy: 0.9717 - val_loss: 0.2923 - val_accuracy: 0.9271\n",
      "Epoch 69/100\n",
      "80000/80000 [==============================] - 33s 414us/sample - loss: 0.0924 - accuracy: 0.9718 - val_loss: 0.2940 - val_accuracy: 0.9268\n",
      "Epoch 70/100\n",
      "80000/80000 [==============================] - 33s 415us/sample - loss: 0.0920 - accuracy: 0.9719 - val_loss: 0.2941 - val_accuracy: 0.9270\n",
      "Epoch 71/100\n",
      "80000/80000 [==============================] - 34s 425us/sample - loss: 0.0916 - accuracy: 0.9720 - val_loss: 0.2957 - val_accuracy: 0.9267\n",
      "Epoch 72/100\n",
      "80000/80000 [==============================] - 34s 422us/sample - loss: 0.0912 - accuracy: 0.9721 - val_loss: 0.2964 - val_accuracy: 0.9268\n",
      "Epoch 73/100\n",
      "80000/80000 [==============================] - 34s 429us/sample - loss: 0.0909 - accuracy: 0.9722 - val_loss: 0.2990 - val_accuracy: 0.9264\n",
      "Epoch 74/100\n",
      "80000/80000 [==============================] - 33s 413us/sample - loss: 0.0906 - accuracy: 0.9723 - val_loss: 0.2988 - val_accuracy: 0.9265\n",
      "Epoch 75/100\n",
      "80000/80000 [==============================] - 33s 417us/sample - loss: 0.0903 - accuracy: 0.9724 - val_loss: 0.3001 - val_accuracy: 0.9265\n",
      "Epoch 76/100\n",
      "80000/80000 [==============================] - 35s 431us/sample - loss: 0.0900 - accuracy: 0.9724 - val_loss: 0.3009 - val_accuracy: 0.9264\n",
      "Epoch 77/100\n",
      "80000/80000 [==============================] - 35s 435us/sample - loss: 0.0896 - accuracy: 0.9726 - val_loss: 0.3019 - val_accuracy: 0.9262\n",
      "Epoch 78/100\n",
      "80000/80000 [==============================] - 36s 454us/sample - loss: 0.0893 - accuracy: 0.9726 - val_loss: 0.3012 - val_accuracy: 0.9264\n",
      "Epoch 79/100\n",
      "80000/80000 [==============================] - 33s 418us/sample - loss: 0.0890 - accuracy: 0.9727 - val_loss: 0.3033 - val_accuracy: 0.9262\n",
      "Epoch 80/100\n",
      "80000/80000 [==============================] - 33s 413us/sample - loss: 0.0887 - accuracy: 0.9727 - val_loss: 0.3042 - val_accuracy: 0.9259\n",
      "Epoch 81/100\n",
      "80000/80000 [==============================] - 33s 410us/sample - loss: 0.0885 - accuracy: 0.9728 - val_loss: 0.3034 - val_accuracy: 0.9260\n",
      "Epoch 82/100\n",
      "80000/80000 [==============================] - 33s 416us/sample - loss: 0.0882 - accuracy: 0.9729 - val_loss: 0.3060 - val_accuracy: 0.9261\n",
      "Epoch 83/100\n",
      "80000/80000 [==============================] - 33s 414us/sample - loss: 0.0906 - accuracy: 0.9723 - val_loss: 0.3044 - val_accuracy: 0.9261\n",
      "Epoch 84/100\n",
      "80000/80000 [==============================] - 33s 412us/sample - loss: 0.0880 - accuracy: 0.9730 - val_loss: 0.3081 - val_accuracy: 0.9258\n",
      "Epoch 85/100\n",
      "80000/80000 [==============================] - 33s 414us/sample - loss: 0.0875 - accuracy: 0.9731 - val_loss: 0.3082 - val_accuracy: 0.9258\n",
      "Epoch 86/100\n",
      "80000/80000 [==============================] - 33s 411us/sample - loss: 0.0872 - accuracy: 0.9732 - val_loss: 0.3069 - val_accuracy: 0.9260\n",
      "Epoch 87/100\n",
      "80000/80000 [==============================] - 33s 415us/sample - loss: 0.0871 - accuracy: 0.9733 - val_loss: 0.3074 - val_accuracy: 0.9262\n",
      "Epoch 88/100\n",
      "80000/80000 [==============================] - 33s 411us/sample - loss: 0.0867 - accuracy: 0.9733 - val_loss: 0.3083 - val_accuracy: 0.9259\n",
      "Epoch 89/100\n",
      "80000/80000 [==============================] - 33s 412us/sample - loss: 0.0883 - accuracy: 0.9729 - val_loss: 0.3108 - val_accuracy: 0.9257\n",
      "Epoch 90/100\n",
      "80000/80000 [==============================] - 33s 413us/sample - loss: 0.0864 - accuracy: 0.9734 - val_loss: 0.3094 - val_accuracy: 0.9259\n",
      "Epoch 91/100\n",
      "80000/80000 [==============================] - 33s 413us/sample - loss: 0.0861 - accuracy: 0.9735 - val_loss: 0.3100 - val_accuracy: 0.9258\n",
      "Epoch 92/100\n",
      "80000/80000 [==============================] - 35s 435us/sample - loss: 0.0859 - accuracy: 0.9735 - val_loss: 0.3119 - val_accuracy: 0.9256\n",
      "Epoch 93/100\n",
      "80000/80000 [==============================] - 34s 423us/sample - loss: 0.0856 - accuracy: 0.9736 - val_loss: 0.3119 - val_accuracy: 0.9256\n",
      "Epoch 94/100\n",
      "80000/80000 [==============================] - 34s 426us/sample - loss: 0.0854 - accuracy: 0.9737 - val_loss: 0.3143 - val_accuracy: 0.9254\n",
      "Epoch 95/100\n",
      "80000/80000 [==============================] - 35s 440us/sample - loss: 0.0852 - accuracy: 0.9737 - val_loss: 0.3137 - val_accuracy: 0.9255\n",
      "Epoch 96/100\n",
      "80000/80000 [==============================] - 36s 446us/sample - loss: 0.0849 - accuracy: 0.9738 - val_loss: 0.3138 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "80000/80000 [==============================] - 33s 415us/sample - loss: 0.0847 - accuracy: 0.9739 - val_loss: 0.3144 - val_accuracy: 0.9254\n",
      "Epoch 98/100\n",
      "80000/80000 [==============================] - 35s 442us/sample - loss: 0.0846 - accuracy: 0.9739 - val_loss: 0.3162 - val_accuracy: 0.9254\n",
      "Epoch 99/100\n",
      "80000/80000 [==============================] - 33s 417us/sample - loss: 0.0848 - accuracy: 0.9738 - val_loss: 0.3159 - val_accuracy: 0.9253\n",
      "Epoch 100/100\n",
      "80000/80000 [==============================] - 35s 440us/sample - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.3178 - val_accuracy: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20fff88cef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 80)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 104)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 345088      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  369664      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 104)    26728       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 741,480\n",
      "Trainable params: 741,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = tensorflow.keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = tensorflow.keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = tensorflow.keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tensorflow.keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the English sentence you want to convert into the German \n",
      "Really?\n"
     ]
    }
   ],
   "source": [
    "input_sentence = input('Enter the English sentence you want to convert into the German \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_translation_input_data = np.zeros(\n",
    "    (1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "#One hot representation\n",
    "for t, char in enumerate(input_sentence): # 't' is the count and char will get sentence in the input_text[0](extracting the charecter one by one)\n",
    "    encoder_translation_input_data[0, t, input_token_index[char]] = 1.0 # assigning 1 to the charecter\n",
    "encoder_translation_input_data[0, t + 1 :, input_token_index[\" \"]] = 1.0 # after each word where ever there is space(' ') then also assigning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 80)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_translation_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Really?\n",
      "Decoded sentence: Wirklich?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = decode_sequence(encoder_translation_input_data)\n",
    "print(\"Input sentence:\", input_sentence)\n",
    "print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
